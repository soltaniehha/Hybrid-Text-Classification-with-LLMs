{"cells":[{"cell_type":"markdown","metadata":{"id":"gutk5P6wAxxQ"},"source":[" # Step 2A1: Synthetic Text Generation\n","\n","\n","\n"," In this step, we generate synthetic tweets to create a balanced dataset for training our classifier. The process includes:\n","\n","\n","\n"," 1. Data Generation:\n","\n","    - Using GPT-4 to generate synthetic tweets\n","\n","    - Creating both literal and sarcastic tweets\n","\n","    - Using original tweets as reference for style and topic\n","\n","    - Maintaining similar length and natural language patterns\n","\n","\n","\n"," 2. Data Processing:\n","\n","    - Cleaning and normalizing text\n","\n","    - Adding metadata (word count, character count, hashtag/mention presence)\n","\n","    - Filtering tweets (minimum 10 words)\n","\n","    - Computing English word statistics\n","\n","\n","\n"," 3. Quality Control:\n","\n","    - Ensuring balanced distribution between literal and sarcastic classes\n","\n","    - Maintaining realistic tweet properties\n","\n","    - Tracking original tweet references\n","\n","    - Computing confidence scores\n","\n","\n","\n"," Note: This step takes about 1.5 hours and costs ~$0.50 to run for 3000 samples per class.\n","\n"," The generated data is saved to `synthetic_tweets.csv` and can be reused to skip this step.\n","\n","\n","\n"," The synthetic dataset will be used to train our initial classifier, which will help us identify and handle label inconsistencies in the original dataset."]},{"cell_type":"markdown","metadata":{"id":"TkEuR9S9AxxS"},"source":[" ## Step 2.1: Create Synthetic Data and Train Naive Classifier\n","\n","\n","\n"," We'll use GPT to generate synthetic tweets for both literal and sarcastic classes to help train our initial classifier.\n","\n"," Before you begin, download Create `openai_api_key.json` with your API key in project root."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rAOfn0nWAxxS","executionInfo":{"status":"ok","timestamp":1747112501483,"user_tz":240,"elapsed":479,"user":{"displayName":"Mohammad Soltanieh Ha","userId":"12308918870841825745"}},"outputId":"8dcbeeef-262a-4c2d-db1e-2fa9c1a66204"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["# Import necessary libraries\n","import os\n","import json\n","import pandas as pd\n","import openai\n","from tqdm import tqdm\n","import time\n","from typing import List, Dict, Tuple\n","import random\n","from nltk.tokenize import word_tokenize\n","import re\n","import unicodedata\n","import nltk\n","nltk.download('punkt', quiet=True)\n","nltk.download('punkt_tab', quiet=True)\n"]},{"cell_type":"code","source":["from google.colab import userdata # Import userdata to access secrets\n","\n","# Load OpenAI API key from Colab secrets\n","openai.api_key = userdata.get('OpenAI_API_Key')\n","\n","# Initialize OpenAI client\n","client = openai.OpenAI(api_key=openai.api_key)"],"metadata":{"id":"PfnZmugdCLOb","executionInfo":{"status":"ok","timestamp":1747112756980,"user_tz":240,"elapsed":533,"user":{"displayName":"Mohammad Soltanieh Ha","userId":"12308918870841825745"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7H3X8jRAxxT","executionInfo":{"status":"ok","timestamp":1747113055352,"user_tz":240,"elapsed":10912,"user":{"displayName":"Mohammad Soltanieh Ha","userId":"12308918870841825745"}},"outputId":"67eee065-ea43-4b36-b3c4-ce8b046aa0f5"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-730402705877>:3: DtypeWarning: Columns (0,1,6,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n","  original_df = pd.read_csv('processed_sentiment140.csv',\n"]}],"source":["# Load the original dataset\n","columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n","original_df = pd.read_csv('processed_sentiment140.csv',\n","                         encoding='latin-1',\n","                         names=columns)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"MMh55IPeAxxT","executionInfo":{"status":"ok","timestamp":1747113076262,"user_tz":240,"elapsed":9,"user":{"displayName":"Mohammad Soltanieh Ha","userId":"12308918870841825745"}}},"outputs":[],"source":["def clean_text(text: str) -> str:\n","    \"\"\"\n","    Clean and normalize text.\n","\n","    Args:\n","        text (str): Input text to clean\n","\n","    Returns:\n","        str: Cleaned text\n","    \"\"\"\n","    # Convert to lowercase\n","    text = text.lower()\n","\n","    # Normalize unicode characters\n","    text = unicodedata.normalize('NFKD', text)\n","\n","    # Remove URLs\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","\n","    # Remove user mentions and hashtags\n","    text = re.sub(r'@\\w+|\\#\\w+', '', text)\n","\n","    # Remove special characters\n","    text = re.sub(r'[^\\w\\s]', ' ', text)\n","\n","    # Remove extra whitespace\n","    text = ' '.join(text.split())\n","\n","    return text\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"vxO7WKbwAxxT","executionInfo":{"status":"ok","timestamp":1747113077329,"user_tz":240,"elapsed":4,"user":{"displayName":"Mohammad Soltanieh Ha","userId":"12308918870841825745"}}},"outputs":[],"source":["def is_english_word(word: str) -> bool:\n","    \"\"\"\n","    Check if a word is English.\n","\n","    Args:\n","        word (str): Word to check\n","\n","    Returns:\n","        bool: True if word is English\n","    \"\"\"\n","    return bool(re.match(r'^[a-zA-Z]+$', word))\n","\n","def count_english_words(text: str) -> int:\n","    \"\"\"\n","    Count the number of English words in text.\n","\n","    Args:\n","        text (str): Input text\n","\n","    Returns:\n","        int: Number of English words\n","    \"\"\"\n","    words = word_tokenize(text)\n","    return sum(1 for word in words if is_english_word(word))\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"nX0ARfUuAxxT","executionInfo":{"status":"ok","timestamp":1747113078159,"user_tz":240,"elapsed":6,"user":{"displayName":"Mohammad Soltanieh Ha","userId":"12308918870841825745"}}},"outputs":[],"source":["def get_tweet_properties(tweet: str) -> Dict:\n","    \"\"\"\n","    Analyze properties of a tweet.\n","\n","    Args:\n","        tweet: The tweet text\n","\n","    Returns:\n","        Dictionary containing tweet properties\n","    \"\"\"\n","    words = word_tokenize(tweet)\n","    return {\n","        'word_count': len(words),\n","        'char_count': len(tweet),\n","        'has_hashtag': int('#' in tweet),\n","        'has_mention': int('@' in tweet)\n","    }\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"04itThJ_AxxT","executionInfo":{"status":"ok","timestamp":1747113078860,"user_tz":240,"elapsed":3,"user":{"displayName":"Mohammad Soltanieh Ha","userId":"12308918870841825745"}}},"outputs":[],"source":["def generate_synthetic_tweet(client: openai.OpenAI,\n","                           original_tweet: str,\n","                           class_type: str,\n","                           max_retries: int = 3) -> str:\n","    \"\"\"\n","    Generate a synthetic tweet based on an original tweet.\n","\n","    Args:\n","        client: OpenAI client instance\n","        original_tweet: The original tweet to use as reference\n","        class_type: Either 'literal' or 'sarcastic'\n","        max_retries: Maximum number of retries for failed API calls\n","\n","    Returns:\n","        Generated tweet text\n","    \"\"\"\n","    prompt = f\"\"\"Given this tweet: \"{original_tweet}\"\n","\n","    Generate a new tweet that:\n","    1. Is about the same subject/topic\n","    2. Has a similar length and style\n","    3. Is {class_type} in tone\n","    4. Sounds natural and realistic\n","\n","    Only return the new tweet text, nothing else.\"\"\"\n","\n","    for attempt in range(max_retries):\n","        try:\n","            response = client.chat.completions.create(\n","                model=\"gpt-4.1-mini\",\n","                messages=[\n","                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates realistic tweets.\"},\n","                    {\"role\": \"user\", \"content\": prompt}\n","                ],\n","                temperature=0.7,\n","                max_tokens=500\n","            )\n","            tweet = response.choices[0].message.content.strip()\n","            if tweet:\n","                return tweet\n","        except Exception as e:\n","            if attempt == max_retries - 1:\n","                print(f\"Failed to generate tweet after {max_retries} attempts: {str(e)}\")\n","            time.sleep(0.5)  # Wait before retrying\n","\n","    return None\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"cTwcy-wYAxxU","executionInfo":{"status":"ok","timestamp":1747113079836,"user_tz":240,"elapsed":4,"user":{"displayName":"Mohammad Soltanieh Ha","userId":"12308918870841825745"}}},"outputs":[],"source":["def generate_synthetic_dataset(client: openai.OpenAI,\n","                             original_df: pd.DataFrame,\n","                             num_samples: int = 10,\n","                             max_retries: int = 3) -> pd.DataFrame:\n","    \"\"\"\n","    Generate synthetic dataset based on original tweets.\n","\n","    Args:\n","        client: OpenAI client instance\n","        original_df: Original dataset DataFrame\n","        num_samples: Number of samples to generate per class\n","        max_retries: Maximum number of retries for failed API calls\n","\n","    Returns:\n","        DataFrame containing synthetic tweets\n","    \"\"\"\n","    synthetic_tweets = []\n","    synthetic_labels = []\n","    original_tweets = []\n","\n","    # Sample random tweets from original dataset\n","    sample_tweets = original_df.sample(n=num_samples*2)['text'].tolist()\n","\n","    # Generate literal tweets\n","    print(\"Generating literal tweets...\")\n","    for original_tweet in tqdm(sample_tweets[:num_samples]):\n","        # Generate synthetic tweets (this may take a while and cost tokens)\n","        synthetic_tweet = generate_synthetic_tweet(client, original_tweet, 'literal', max_retries)\n","        if synthetic_tweet:\n","            synthetic_tweets.append(synthetic_tweet)\n","            synthetic_labels.append('literal')\n","            original_tweets.append(original_tweet)\n","\n","    # Generate sarcastic tweets\n","    print(\"Generating sarcastic tweets...\")\n","    for original_tweet in tqdm(sample_tweets[num_samples:]):\n","        # Generate synthetic tweets (this may take a while and cost tokens)\n","        synthetic_tweet = generate_synthetic_tweet(client, original_tweet, 'sarcastic', max_retries)\n","        if synthetic_tweet:\n","            synthetic_tweets.append(synthetic_tweet)\n","            synthetic_labels.append('sarcastic')\n","            original_tweets.append(original_tweet)\n","\n","    # Create DataFrame\n","    synthetic_data = pd.DataFrame({\n","        'text': synthetic_tweets,\n","        'label': synthetic_labels,\n","        'original_tweet': original_tweets,\n","        'source': ['synthetic'] * len(synthetic_tweets),\n","        'confidence': [1.0] * len(synthetic_tweets)\n","    })\n","\n","    # Add metadata columns\n","    synthetic_data['word_count'] = synthetic_data['text'].str.split().str.len()\n","    synthetic_data['char_count'] = synthetic_data['text'].str.len()\n","    synthetic_data['has_hashtag'] = synthetic_data['text'].str.contains('#').astype(int)\n","    synthetic_data['has_mention'] = synthetic_data['text'].str.contains('@').astype(int)\n","\n","    return synthetic_data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7O26_uAAxxU"},"outputs":[],"source":["# Generate synthetic dataset\n","print(\"Generating synthetic dataset...\")\n","synthetic_data = generate_synthetic_dataset(client, original_df, num_samples=3000)\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XDJDm8wtAxxU","executionInfo":{"status":"ok","timestamp":1747113353472,"user_tz":240,"elapsed":1064,"user":{"displayName":"Mohammad Soltanieh Ha","userId":"12308918870841825745"}},"outputId":"fe162bce-0152-4627-da95-ee2e7d317a9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Preprocessing synthetic dataset...\n","Starting synthetic dataset preprocessing...\n","Cleaning text...\n","Adding word count information...\n","Original synthetic dataset size: 6000\n","Filtered synthetic dataset size: 5003\n","Removed 997 tweets with fewer than 10 words\n","Average word count: 18.42\n","Average English word count: 18.24\n"]}],"source":["# Preprocess the synthetic dataset\n","print(\"\\nPreprocessing synthetic dataset...\")\n","print(\"Starting synthetic dataset preprocessing...\")\n","\n","# Clean text\n","print(\"Cleaning text...\")\n","synthetic_data['cleaned_text'] = synthetic_data['text'].apply(clean_text)\n","\n","# Add word count information\n","print(\"Adding word count information...\")\n","synthetic_data['word_count'] = synthetic_data['cleaned_text'].apply(lambda x: len(word_tokenize(x)))\n","synthetic_data['english_word_count'] = synthetic_data['cleaned_text'].apply(count_english_words)\n","\n","# Filter out tweets with fewer than 10 words\n","original_size = len(synthetic_data)\n","synthetic_data = synthetic_data[synthetic_data['word_count'] >= 10]\n","filtered_size = len(synthetic_data)\n","\n","# Print statistics\n","print(f\"Original synthetic dataset size: {original_size}\")\n","print(f\"Filtered synthetic dataset size: {filtered_size}\")\n","print(f\"Removed {original_size - filtered_size} tweets with fewer than 10 words\")\n","print(f\"Average word count: {synthetic_data['word_count'].mean():.2f}\")\n","print(f\"Average English word count: {synthetic_data['english_word_count'].mean():.2f}\")\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oJEvZc62AxxU","executionInfo":{"status":"ok","timestamp":1747113368740,"user_tz":240,"elapsed":82,"user":{"displayName":"Mohammad Soltanieh Ha","userId":"12308918870841825745"}},"outputId":"7dd5ac9c-5ef5-4031-a0ff-f04dfbc96b6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Synthetic data saved to 'synthetic_tweets.csv'\n"]}],"source":["# Save synthetic data\n","synthetic_data.to_csv('synthetic_tweets.csv', index=False)\n","print(\"Synthetic data saved to 'synthetic_tweets.csv'\")\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wijB4JKgAxxU","executionInfo":{"status":"ok","timestamp":1747113370847,"user_tz":240,"elapsed":52,"user":{"displayName":"Mohammad Soltanieh Ha","userId":"12308918870841825745"}},"outputId":"f3b9f315-04cd-48a6-af93-21cd3b3c0cfc"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Synthetic Data Statistics:\n","Total samples: 5003\n","\n","Label distribution:\n","label\n","sarcastic    2754\n","literal      2249\n","Name: count, dtype: int64\n","\n","Text length statistics by label:\n","            count       mean       std   min   25%   50%   75%   max\n","label                                                               \n","literal    2249.0  17.824366  5.688833  10.0  13.0  17.0  22.0  36.0\n","sarcastic  2754.0  18.915033  6.187649  10.0  14.0  18.0  23.0  55.0\n","\n","Hashtag and mention usage by label:\n","           has_hashtag  has_mention\n","label                              \n","literal       0.020898     0.448199\n","sarcastic     0.074074     0.387073\n"]}],"source":["# Display statistics\n","print(\"\\nSynthetic Data Statistics:\")\n","print(f\"Total samples: {len(synthetic_data)}\")\n","print(\"\\nLabel distribution:\")\n","print(synthetic_data['label'].value_counts())\n","\n","print(\"\\nText length statistics by label:\")\n","print(synthetic_data.groupby('label')['word_count'].describe())\n","\n","print(\"\\nHashtag and mention usage by label:\")\n","print(synthetic_data.groupby('label')[['has_hashtag', 'has_mention']].mean())\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"17VOeKK2AxxU","executionInfo":{"status":"ok","timestamp":1747113375819,"user_tz":240,"elapsed":244,"user":{"displayName":"Mohammad Soltanieh Ha","userId":"12308918870841825745"}},"outputId":"7ac25df8-6770-441c-bba5-03e6b27844a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Sample pairs of original and synthetic tweets:\n","\n","Original tweet: Watching jordon walk into his doom mwhahaha \n","Synthetic tweet (literal): Seeing Jordan head straight into trouble, can’t wait to see what happens next.\n","Cleaned synthetic tweet: seeing jordan head straight into trouble can t wait to see what happens next\n","--------------------------------------------------------------------------------\n","\n","Original tweet: Soo hungry !!, but why am I still in bed?  Got cookie monster!?  \n","Synthetic tweet (literal): Starving right now, but why can't I get out of bed? Where’s my cookie monster?\n","Cleaned synthetic tweet: starving right now but why can t i get out of bed where s my cookie monster\n","--------------------------------------------------------------------------------\n","\n","Original tweet: Gotta go now.  Parent alert again. I currently have 160k and proud about climbing my way up from 80k to 160 in just 4hours. &lt;33 Nighty! :*\n","Synthetic tweet (literal): Time to head out, parents just checked in. So happy I doubled my count from 80k to 160k in just 4 hours. <3 Catch you later! :)\n","Cleaned synthetic tweet: time to head out parents just checked in so happy i doubled my count from 80k to 160k in just 4 hours 3 catch you later\n","--------------------------------------------------------------------------------\n","\n","Original tweet: @ritalavalerie, you know, actually, from the moment you introduced those slims to me, I've become addicted to them xD they're gorgeous \n","Synthetic tweet (literal): @ritalavalerie Honestly, ever since you showed me those slims, I can't get enough of them. They’re seriously amazing!\n","Cleaned synthetic tweet: honestly ever since you showed me those slims i can t get enough of them they re seriously amazing\n","--------------------------------------------------------------------------------\n","\n","Original tweet: They gave me 2 drips and a shoot in my a$$ &amp; I stell have a freakin headch but the tommy pain is gone, thank god now I can dreank watter  \n","Synthetic tweet (literal): Got two IV drips and a shot in my butt, still got this annoying headache, but the stomach pain is finally gone. Thank goodness, now I can drink water without issues.\n","Cleaned synthetic tweet: got two iv drips and a shot in my butt still got this annoying headache but the stomach pain is finally gone thank goodness now i can drink water without issues\n","--------------------------------------------------------------------------------\n"]}],"source":["# Display sample pairs of original and synthetic tweets\n","print(\"\\nSample pairs of original and synthetic tweets:\")\n","sample_pairs = synthetic_data[['original_tweet', 'text', 'cleaned_text', 'label']].head(5)\n","for _, row in sample_pairs.iterrows():\n","    print(f\"\\nOriginal tweet: {row['original_tweet']}\")\n","    print(f\"Synthetic tweet ({row['label']}): {row['text']}\")\n","    print(f\"Cleaned synthetic tweet: {row['cleaned_text']}\")\n","    print(\"-\" * 80)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ssspEdxNAxxU"},"outputs":[],"source":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}